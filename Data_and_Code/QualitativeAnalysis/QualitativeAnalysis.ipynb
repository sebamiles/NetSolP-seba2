{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpHfujKirGQq"
   },
   "outputs": [],
   "source": [
    "%pip install captum\n",
    "%pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izUAXltftij6"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjRIV8k2rYcp"
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VBGpwy79gG-e"
   },
   "outputs": [],
   "source": [
    "# @title finetuning model mean\n",
    "class ESMFinetune(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model, alphabet = pretrained.load_model_and_alphabet(\"esm1_t12_85M_UR50S\")\n",
    "        self.model = model\n",
    "        self.clf_head = nn.Linear(768, 1)\n",
    "\n",
    "        # The ESM 12 model does not have a layer norm before MLM. Therefore the 768 feature output has spikes.\n",
    "        # We found no difference in performance by adding this. \n",
    "        with open(\"../Training/ESM12_Layer12_Norm.pkl\", \"rb\") as f:\n",
    "            final_scaling = pickle.load(f)\n",
    "        self.scaling_mean = torch.tensor(final_scaling[\"mean\"], device=\"cuda\", requires_grad=False)\n",
    "        self.scaling_std = torch.tensor(final_scaling[\"std\"], device=\"cuda\", requires_grad=False)\n",
    "        self.final_ln = nn.LayerNorm(768)\n",
    "        self.lr = 2e-5\n",
    "    def forward(self, toks, lens, non_mask):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        x = self.model(toks, repr_layers=[12])\n",
    "        x = x[\"representations\"][12]\n",
    "        x = (x- self.scaling_mean) / self.scaling_std\n",
    "        x = self.final_ln(x)\n",
    "        x_mean = (x * non_mask[:,:,None]).sum(1) / lens[:,None]\n",
    "        x = self.clf_head(x_mean)\n",
    "        return x.squeeze() \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.model.named_parameters()], 'lr': 3e-6},\n",
    "            {\"params\": [p for n, p in self.clf_head.named_parameters()] + [p for n, p in self.final_ln.named_parameters()], 'lr': 2e-5},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(grouped_parameters, lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #self.unfreeze()\n",
    "        x, l, n, y, _ = batch\n",
    "        y_pred =  self.forward(x, l, n)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y)\n",
    "        self.log('train_loss_batch', loss)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('train_loss', avg_loss, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #self.freeze()\n",
    "        x, l, n, y, _ = batch\n",
    "        y_pred =  self.forward(x, l, n)\n",
    "        correct = ((y_pred>0) == y).sum()\n",
    "        count = y.size(0)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y)\n",
    "        self.log('val_loss_batch', loss)\n",
    "        return {'loss': loss, 'correct':correct, \"count\":count}\n",
    "  \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('val_loss', avg_loss, prog_bar=True)\n",
    "        avg_acc = torch.tensor([x['correct'] for x in outputs]).sum() / torch.tensor([x['count'] for x in outputs]).sum()\n",
    "        self.log('val_acc', avg_acc, prog_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6RVDJ7MteFC"
   },
   "source": [
    "# **Attribution Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hv9vHUfZrYxY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# reference https://captum.ai/tutorials/Bert_SQUAD_Interpret\n",
    "def predict(toks, lengths, np_mask):\n",
    "    return clf(toks, lengths, np_mask)\n",
    "\n",
    "def custom_forward(toks, lengths, np_mask):\n",
    "    preds = predict(toks, lengths, np_mask)\n",
    "    return torch.sigmoid(preds)\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "for split_i in range(5):\n",
    "    print(split_i)\n",
    "    path = f\"./models/{split_i}PSISplit.ckpt\"\n",
    "    clf = ESMFinetune.load_from_checkpoint(path).cuda()\n",
    "    clf.zero_grad()\n",
    "    lig = LayerIntegratedGradients(custom_forward, clf.model.embed_tokens)\n",
    "\n",
    "    data_df = pd.read_csv(\"../Datasets/NESG/NESG_testset.csv\")\n",
    "    data_df = pd.DataFrame(fasta_dict.items(), columns=['sid', 'fasta'])\n",
    "\n",
    "    newalphabet = NewAlphabet(alphabet)\n",
    "    embed_dataset = FastaBatchedDataset(data_df)\n",
    "    embed_batches = embed_dataset.get_batch_indices(2048, extra_toks_per_seq=1)\n",
    "    embed_dataloader = torch.utils.data.DataLoader(embed_dataset, collate_fn=newalphabet.get_batch_converter(), batch_sampler=embed_batches)\n",
    "\n",
    "    score_vises_dict = {}\n",
    "    attribution_dict = {}\n",
    "    pred_dict = {}\n",
    "    for j, (toks, lengths, np_mask, labels) in enumerate(embed_dataloader):\n",
    "        #print(toks.shape)\n",
    "        if j % 10 == 0:\n",
    "          print(j, \"/\", len(embed_dataloader))\n",
    "        baseline_toks = torch.empty((toks.size(0), toks.size(1)), dtype=torch.int64)\n",
    "        baseline_toks.fill_(newalphabet.alphabet.padding_idx)\n",
    "        baseline_toks[:, 0] = newalphabet.alphabet.cls_idx\n",
    "        attributions, delta = lig.attribute(inputs=toks.to(\"cuda\"),\n",
    "                                        baselines=baseline_toks.to(\"cuda\"),\n",
    "                                        n_steps=50,\n",
    "                                        additional_forward_args=(lengths.to(\"cuda\"), np_mask.to(\"cuda\")),\n",
    "                                        internal_batch_size=8,\n",
    "                                        return_convergence_delta=True)\n",
    "        \n",
    "        preds = custom_forward(toks.to(\"cuda\"),lengths.to(\"cuda\"), np_mask.to(\"cuda\"))\n",
    "\n",
    "        for i in range(preds.shape[0]):\n",
    "            #attributions_sum = summarize_attributions(attributions[i])\n",
    "            attribution_dict[labels[i]] = attributions[i].sum(dim=-1).squeeze(0).cpu().numpy()[1:1+lengths[i]]\n",
    "            pred_dict[labels[i]] = preds[i].cpu().detach().numpy()\n",
    "\n",
    "    #print(attribution_dict)\n",
    "    with open(f\"{split_i}_attrs.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"attributions\": attribution_dict, \"preds\": pred_dict}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeOF-TdWc1Ni"
   },
   "outputs": [],
   "source": [
    "\n",
    "attr_dict = {}\n",
    "pred_dict = {}\n",
    "for i in range(5):    \n",
    "    with open(f\"{split_i}_attrs.pkl\", \"rb\") as f:\n",
    "        attrs = pickle.load(f)\n",
    "    for k in attrs['attributions']:\n",
    "        if k in attr_dict:\n",
    "            attr_dict[k] += attrs['attributions'][k]\n",
    "        else:\n",
    "            attr_dict[k] = attrs['attributions'][k]\n",
    "    for k in attrs['preds']:\n",
    "        if k in pred_dict:\n",
    "            pred_dict[k] += attrs['preds'][k]\n",
    "        else:\n",
    "            pred_dict[k] = attrs['preds'][k]\n",
    "\n",
    "for k in attr_dict:\n",
    "    attr_dict[k] = attr_dict[k] / np.abs(attr_dict[k]).sum()\n",
    "\n",
    "for k in attr_dict:\n",
    "    pred_dict[k] = pred_dict[k] / 5\n",
    "\n",
    "def read_fasta(fastafile):\n",
    "    \"\"\"Parse a file with sequences in FASTA format and store in a dict\"\"\"\n",
    "    with open(fastafile, 'r') as f:\n",
    "        content = [l.strip() for l in f.readlines()]\n",
    "\n",
    "    res = {}\n",
    "    seq, seq_id = '', None\n",
    "    for line in content:\n",
    "        if line.startswith('>'):\n",
    "            \n",
    "            if len(seq) > 0:\n",
    "                res[seq_id] = seq\n",
    "            \n",
    "            seq_id = line.replace('>', '')\n",
    "            seq = ''\n",
    "        else:\n",
    "            seq += line\n",
    "    res[seq_id] = seq\n",
    "    return res\n",
    "seq_dict = read_fasta(f\"../Datasets/NESG/NESG_testset.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OttPzWhes7f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5vk05aPetbV"
   },
   "source": [
    "# **Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABMmbLezewU4"
   },
   "outputs": [],
   "source": [
    "start_imp = []\n",
    "def length_avg(attrs):\n",
    "    sum_dict = {i:0 for i in range(0,101)}\n",
    "    count_dict = {i:0 for i in range(0,101)}\n",
    "    data_dict = {i:[] for i in range(0,101)}\n",
    "    for k in attrs:\n",
    "        if abs(attrs[k][0]) > 0.005:\n",
    "            start_imp.append(k)\n",
    "        for j in range(attrs[k].shape[0]):\n",
    "            bin_val = int(((j)/attrs[k].shape[0]) * 100)\n",
    "            sum_dict[bin_val] += abs(attrs[k][j])\n",
    "            count_dict[bin_val] += 1\n",
    "            data_dict[bin_val].append(abs(attrs[k][j]))\n",
    "    return sum_dict, count_dict, data_dict\n",
    "\n",
    "def length_avg2(attrs):\n",
    "    sum_dict = {i:0 for i in range(0,101)}\n",
    "    count_dict = {i:0 for i in range(0,101)}\n",
    "    data_dict = {i:[] for i in range(0,101)}\n",
    "    for k in attrs:\n",
    "        bin_size = int(100 / attrs[k].shape[0])\n",
    "        for j in range(attrs[k].shape[0]):\n",
    "            bin_val = int((j/attrs[k].shape[0]) * 100)\n",
    "            if bin_size > 1:\n",
    "                for b in range(bin_size):\n",
    "                    sum_dict[bin_val + b] += abs(attrs[k][j]) / bin_size\n",
    "                    count_dict[bin_val + b] += 1 / bin_size\n",
    "                    data_dict[bin_val + b].append(abs(attrs[k][j]) / bin_size)\n",
    "            else:\n",
    "                sum_dict[bin_val] += abs(attrs[k][j])\n",
    "                count_dict[bin_val] += 1\n",
    "                data_dict[bin_val].append(abs(attrs[k][j]))\n",
    "    return sum_dict, count_dict, data_dict\n",
    "\n",
    "def length_avg_abs(attrs):\n",
    "\n",
    "    data_dict = {}\n",
    "    for k in attrs:\n",
    "        for j in range(attrs[k].shape[0]):\n",
    "            if j in data_dict:\n",
    "                data_dict[j].append(abs(attrs[k][j]))\n",
    "            else:\n",
    "                data_dict[j] = [abs(attrs[k][j])]\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def length_avg_label(attrs, label):\n",
    "    sum_dict = {i:0 for i in range(0,101)}\n",
    "    count_dict = {i:0 for i in range(0,101)}\n",
    "    data_dict = {i:[] for i in range(0,101)}\n",
    "    for k in attrs:\n",
    "        if label_df[label_df.sid == k].solubility.item() == label:\n",
    "            for j in range(attrs[k].shape[0]):\n",
    "                bin_val = int((j/attrs[k].shape[0]) * 100)\n",
    "                sum_dict[bin_val] += abs(attrs[k][j])\n",
    "                count_dict[bin_val] += 1\n",
    "                data_dict[bin_val].append(abs(attrs[k][j]))\n",
    "    return sum_dict, count_dict, data_dict\n",
    "\n",
    "swi_weights = {'A': 0.8356471476582918,\n",
    "           'C': 0.5208088354857734,\n",
    "           'E': 0.9876987431418378,\n",
    "           'D': 0.9079044671339564,\n",
    "           'G': 0.7997168496420723,\n",
    "           'F': 0.5849790194237692,\n",
    "           'I': 0.6784124413866582,\n",
    "           'H': 0.8947913996466419,\n",
    "           'K': 0.9267104557513497,\n",
    "           'M': 0.6296623675420369,\n",
    "           'L': 0.6554221515081433,\n",
    "           'N': 0.8597433107431216,\n",
    "           'Q': 0.789434648348208,\n",
    "           'P': 0.8235328714705341,\n",
    "           'S': 0.7440908318492778,\n",
    "           'R': 0.7712466317693457,\n",
    "           'T': 0.8096922697856334,\n",
    "           'W': 0.6374678690957594,\n",
    "           'V': 0.7357837119163659,\n",
    "           'Y': 0.6112801822947587}\n",
    "\n",
    "def aa_avg(attrs, seqs):\n",
    "    sum_dict = {i:0 for i in swi_weights}\n",
    "    count_dict = {i:0 for i in swi_weights}\n",
    "    for k in attrs:\n",
    "        assert attrs[k].shape[0] == len(seqs[k])\n",
    "        for j in range(attrs[k].shape[0]):\n",
    "            bin_val = seqs[k][j]\n",
    "            if bin_val not in swi_weights:\n",
    "                continue\n",
    "            sum_dict[bin_val] += attrs[k][j]\n",
    "            count_dict[bin_val] += 1\n",
    "    return sum_dict, count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0_modFQe0B-"
   },
   "outputs": [],
   "source": [
    "len_sum, len_count, len_list = length_avg(attrs)\n",
    "netsol_lengths = {k: len_sum[k]/(len_count[k] + 1e-5) for k in len_sum}\n",
    "netsol_stds =  {k: np.array(len_list[k]).std() for k in len_sum}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(list(netsol_lengths.keys())[:-1], list(netsol_lengths.values())[:-1])\n",
    "y = np.array(list(netsol_lengths.values())[:-1])\n",
    "error = np.array(list(netsol_stds.values())[:-1])\n",
    "plt.fill_between(list(netsol_lengths.keys())[:-1], y-error, y+error, alpha = 0.3)\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Position as % of length')\n",
    "plt.title(\"Importance vs Length\")\n",
    "fig.savefig('importancevlength.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VrIQciZfQz4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "aa_sum, aa_count = aa_avg(attrs, seq_dict)\n",
    "netsol_scores = {k: aa_sum[k]/aa_count[k] for k in aa_sum}\n",
    "\n",
    "print(spearmanr(list(netsol_scores.values()), list(swi_weights.values())))\n",
    "\n",
    "minx = min(list(netsol_scores.values()))\n",
    "maxx = max(list(netsol_scores.values()))\n",
    "miny = min(list(swi_weights.values()))\n",
    "maxy = max(list(swi_weights.values()))\n",
    "\n",
    "x = np.linspace(minx,maxx,100)\n",
    "y = (miny-maxy) / (minx - maxx) * (x - minx) + miny\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(netsol_scores.values(), swi_weights.values())\n",
    "\n",
    "for i, txt in enumerate(netsol_scores.keys()):\n",
    "    ax.annotate(txt, (list(netsol_scores.values())[i] + 0.0001, list(swi_weights.values())[i] + 0.01))\n",
    "\n",
    "ax.plot(x, y, '-r', label='y=2x+1', alpha=0.5)\n",
    "ax.set_xlabel(\"NetSolP Scores\")\n",
    "ax.set_ylabel(\"SWI Scores\")\n",
    "ax.set_ylim(miny-0.05, maxy+0.05)\n",
    "ax.set_title(\"Amino acid score comparison\")\n",
    "ax.grid()\n",
    "plt.savefig('scorevsaminoacid_scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVDSOHNCtxNA"
   },
   "source": [
    "# **Helpers for formatting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19D4SHCOuXAy"
   },
   "outputs": [],
   "source": [
    "# helpful for choosing the ideal color range\n",
    "max_attr = [x.max() for x in attribution_dict.values()]\n",
    "min_attr = [x.min() for x in attribution_dict.values()]\n",
    "print(max(max_attr), min(min_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PueAh3GYsX3F"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "\n",
    "def _get_color(attr):\n",
    "    # clip values to prevent CSS errors (Values should be from [-1,1])\n",
    "    attr = max(-1, min(1, attr))\n",
    "    if attr > 0:\n",
    "        hue = 120\n",
    "        sat = 75\n",
    "        # Attributions lie between -1 and 1 but for better coloring change scaling based on dataset\n",
    "        lig = 100 - int(80 * attr)\n",
    "    else:\n",
    "        hue = 0\n",
    "        sat = 75\n",
    "        lig = 100 - int(-100 * attr)\n",
    "    return \"hsl({}, {}%, {}%)\".format(hue, sat, lig)\n",
    "\n",
    "def _get_color_sol(attr):\n",
    "    if attr > 0:\n",
    "        hue = 240\n",
    "        sat = 100\n",
    "        lig = 95\n",
    "    else:\n",
    "        hue = 30\n",
    "        sat = 100\n",
    "        lig = 95\n",
    "    return \"hsl({}, {}%, {}%)\".format(hue, sat, lig)\n",
    "\n",
    "def format_word_importances(words, importances, solubility):\n",
    "    tags = [\"<td nowrap>\"]\n",
    "    idx = 0\n",
    "    for word in words:\n",
    "        if word == \"-\":\n",
    "            color = _get_color_sol(solubility)\n",
    "            # We ignore the - character by setting opacity to 0\n",
    "            unwrapped_tag = '<mark style=\"background-color: {color}; opacity:0.0; \\\n",
    "                        line-height:1.75\"><font color=\"black\"> {word}\\\n",
    "                        </font></mark>'.format(\n",
    "                color=color, word=word\n",
    "            )\n",
    "        else:\n",
    "            color = _get_color(importances[idx])\n",
    "            idx += 1\n",
    "            unwrapped_tag = '<mark style=\"background-color: {color}; opacity:1.0; \\\n",
    "                        line-height:1.75\"><font color=\"black\"> {word}\\\n",
    "                        </font></mark>'.format(\n",
    "                color=color, word=word\n",
    "            )\n",
    "        tags.append(unwrapped_tag)\n",
    "\n",
    "    tags.append(\"</td>\")\n",
    "    return \"\".join(tags)\n",
    "\n",
    "def format_classname(s, t=-1):\n",
    "    if t==1:\n",
    "      unwrapped_tag = '<mark style=\"background-color: {color}; opacity:1.0; \\\n",
    "                        line-height:1.75\"><font color=\"black\"> {word}\\\n",
    "                        </font></mark>'.format(color=_get_color(0.3), word=s)\n",
    "      return f\"<td>{unwrapped_tag}</td>\"\n",
    "    elif t==0:\n",
    "      unwrapped_tag = '<mark style=\"background-color: {color}; opacity:1.0; \\\n",
    "                        line-height:1.75\"><font color=\"black\"> {word}\\\n",
    "                        </font></mark>'.format(color=_get_color(-0.3), word=s)\n",
    "      return f\"<td>{unwrapped_tag}</td>\"\n",
    "    else:\n",
    "      return f\"<td>{s}</td>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olS-tE3VsX5r"
   },
   "outputs": [],
   "source": [
    "dom = ['<table style=\"font-family:\\'Courier New\\', monospace\" width: 100%>']\n",
    "rows = [\n",
    "    \"<tr>\"\n",
    "    \"<th>ID</th>\"\n",
    "    \"<th>Label</th>\"\n",
    "    \"<th>Prediction</th>\"\n",
    "    \"<th>MSA</th>\"\n",
    "    \"</tr>\"\n",
    "]\n",
    "\n",
    "for idx in range(len(msa_tsv)):\n",
    "    rows.append(\n",
    "        \"\".join(\n",
    "            [\n",
    "                \"<tr>\",\n",
    "                format_classname(msa_tsv.sid[idx], msa_tsv.solubility[idx]),\n",
    "                format_classname(msa_tsv.solubility[idx]),\n",
    "                format_classname(round(pred_dict[msa_tsv.sid[idx]].item(), 3)),\n",
    "                format_word_importances(msa_tsv.msa[idx], attribution_dict[msa_tsv.sid[idx]], msa_tsv.solubility[idx]),\n",
    "                \"<tr>\",\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "dom.append(\"\".join(rows))\n",
    "dom.append(\"</table>\")\n",
    "html = HTML(\"\".join(dom))\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1Y_REiWsX8U"
   },
   "outputs": [],
   "source": [
    "html_file= open(\"FILENAME\",\"w\")\n",
    "html_file.write(\"\".join(dom))\n",
    "html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbp9wIYSsX_M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zVDSOHNCtxNA"
   ],
   "name": "QualitativeAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
