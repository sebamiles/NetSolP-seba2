{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4K12fKf8WZBo"
   },
   "outputs": [],
   "source": [
    "%pip install pytorch-lightning\n",
    "%pip install git+https://github.com/facebookresearch/esm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKWC9VG_bHXG"
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "EMBEDDING_SIZE = 768\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 510 # for fixed max length batching\n",
    "MAX_TOKENS_PER_BATCH = 4096 # for dynamic batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxC8bj7tbX5-"
   },
   "source": [
    "# **Imports and Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emFJoLhBYCWi"
   },
   "outputs": [],
   "source": [
    "from esm import Alphabet, FastaBatchedDataset, pretrained\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle\n",
    "\n",
    "def read_fasta(fastafile):\n",
    "    \"\"\"Parse a file with sequences in FASTA format and store in a dict\"\"\"\n",
    "    with open(fastafile, 'r') as f:\n",
    "        content = [l.strip() for l in f.readlines()]\n",
    "\n",
    "    res = {}\n",
    "    seq, seq_id = '', None\n",
    "    for line in content:\n",
    "        if line.startswith('>'):\n",
    "            \n",
    "            if len(seq) > 0:\n",
    "                res[seq_id] = seq\n",
    "            \n",
    "            seq_id = line.replace('>', '')\n",
    "            seq = ''\n",
    "        else:\n",
    "            seq += line\n",
    "    res[seq_id] = seq\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbJhKM5zYCZo"
   },
   "outputs": [],
   "source": [
    "# title finetuning model mean\n",
    "\n",
    "class ESMFinetune(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model, alphabet = pretrained.load_model_and_alphabet(\"esm1_t12_85M_UR50S\")\n",
    "        self.model = model\n",
    "        self.clf_head = nn.Linear(768, 1)\n",
    "\n",
    "        # The ESM 12 model does not have a layer norm before MLM. Therefore the 768 feature output has spikes.\n",
    "        # We found no difference in performance by adding this. \n",
    "        with open(\"ESM12_Layer12_Norm.pkl\", \"rb\") as f:\n",
    "            final_scaling = pickle.load(f)\n",
    "        self.scaling_mean = torch.tensor(final_scaling[\"mean\"], device=\"cuda\", requires_grad=False)\n",
    "        self.scaling_std = torch.tensor(final_scaling[\"std\"], device=\"cuda\", requires_grad=False)\n",
    "        self.final_ln = nn.LayerNorm(768)\n",
    "        self.lr = 2e-5\n",
    "    def forward(self, toks, lens, non_mask):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        x = self.model(toks, repr_layers=[12])\n",
    "        x = x[\"representations\"][12]\n",
    "        x = (x- self.scaling_mean) / self.scaling_std\n",
    "        x = self.final_ln(x)\n",
    "        x_mean = (x * non_mask[:,:,None]).sum(1) / lens[:,None]\n",
    "        x = self.clf_head(x_mean)\n",
    "        return x.squeeze() \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.model.named_parameters()], 'lr': 3e-6},\n",
    "            {\"params\": [p for n, p in self.clf_head.named_parameters()] + [p for n, p in self.final_ln.named_parameters()], 'lr': 2e-5},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(grouped_parameters, lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #self.unfreeze()\n",
    "        x, l, n, y, _ = batch\n",
    "        y_pred =  self.forward(x, l, n)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y)\n",
    "        self.log('train_loss_batch', loss)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('train_loss', avg_loss, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #self.freeze()\n",
    "        x, l, n, y, _ = batch\n",
    "        y_pred =  self.forward(x, l, n)\n",
    "        correct = ((y_pred>0) == y).sum()\n",
    "        count = y.size(0)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y)\n",
    "        self.log('val_loss_batch', loss)\n",
    "        return {'loss': loss, 'correct':correct, \"count\":count}\n",
    "  \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('val_loss', avg_loss, prog_bar=True)\n",
    "        avg_acc = torch.tensor([x['correct'] for x in outputs]).sum() / torch.tensor([x['count'] for x in outputs]).sum()\n",
    "        self.log('val_acc', avg_acc, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpLfKPFaYCih"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giHbgUFGWlwg"
   },
   "source": [
    "# **TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEG1QIJ7Ynxn"
   },
   "source": [
    "Pick one for training: Dynamic batching or fixed max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "YARHvw4XWeJD"
   },
   "outputs": [],
   "source": [
    "# @title Dynamic batching\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class FastaBatchedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.data_df = self.data_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df[\"fasta\"][idx], self.data_df[\"solubility\"][idx], self.data_df[\"sid\"][idx]\n",
    "\n",
    "    def get_batch_indices(self, toks_per_batch, extra_toks_per_seq=0):\n",
    "        sizes = [(len(s), i) for i, s in enumerate(self.data_df[\"fasta\"])]\n",
    "        sizes.sort()\n",
    "        batches = []\n",
    "        buf = []\n",
    "        max_len = 0\n",
    "\n",
    "        def _flush_current_buf():\n",
    "            nonlocal max_len, buf\n",
    "            if len(buf) == 0:\n",
    "                return\n",
    "            batches.append(buf)\n",
    "            buf = []\n",
    "            max_len = 0\n",
    "        start = 0\n",
    "        #start = random.randint(0, len(sizes))\n",
    "        for j in range(len(sizes)):\n",
    "            i = (start + j) % len(sizes)\n",
    "            sz = sizes[i][0]\n",
    "            idx = sizes[i][1]    \n",
    "            sz += extra_toks_per_seq\n",
    "            if (max(sz, max_len) * (len(buf) + 1) > toks_per_batch):\n",
    "                _flush_current_buf()\n",
    "            max_len = max(max_len, sz)\n",
    "            buf.append(idx)\n",
    "\n",
    "        _flush_current_buf()\n",
    "        return batches\n",
    "\n",
    "class BatchConverter(object):\n",
    "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "    def __call__(self, raw_batch):\n",
    "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
    "        batch_size = len(raw_batch)\n",
    "        #print(len(raw_batch[0]), raw_batch[1], raw_batch[2])\n",
    "        max_len = max(len(seq_str) for seq_str, _, _ in raw_batch)\n",
    "        tokens = torch.empty((batch_size, max_len + int(self.alphabet.prepend_bos) + \\\n",
    "            int(self.alphabet.append_eos)), dtype=torch.int64)\n",
    "        tokens.fill_(self.alphabet.padding_idx)\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        strs = []\n",
    "        targets = torch.zeros((batch_size,), dtype=torch.float32)\n",
    "        for i, (seq_str, target, label) in enumerate(raw_batch):\n",
    "            #seq_str = seq_str[1:]\n",
    "            labels.append(label)\n",
    "            lengths.append(len(seq_str))\n",
    "            strs.append(seq_str)\n",
    "            targets[i] = target\n",
    "            if self.alphabet.prepend_bos:\n",
    "                tokens[i, 0] = self.alphabet.cls_idx\n",
    "            seq = torch.tensor([self.alphabet.get_idx(s) for s in seq_str], dtype=torch.int64)\n",
    "            tokens[i, int(self.alphabet.prepend_bos) : len(seq_str) + int(self.alphabet.prepend_bos)] = seq\n",
    "            if self.alphabet.append_eos:\n",
    "                tokens[i, len(seq_str) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx\n",
    "        \n",
    "        non_pad_mask = ~tokens.eq(self.alphabet.padding_idx) &\\\n",
    "         ~tokens.eq(self.alphabet.cls_idx) &\\\n",
    "         ~tokens.eq(self.alphabet.eos_idx)# B, T\n",
    "\n",
    "        return tokens, torch.tensor(lengths), non_pad_mask, targets, labels\n",
    "\n",
    "class Alphabet(object):\n",
    "    prepend_toks = (\"<null_0>\", \"<pad>\", \"<eos>\", \"<unk>\")\n",
    "    append_toks = (\"<cls>\", \"<mask>\", \"<sep>\")\n",
    "    prepend_bos = True\n",
    "    append_eos = False\n",
    "\n",
    "    def __init__(self, standard_toks):\n",
    "        self.standard_toks = list(standard_toks)\n",
    "\n",
    "        self.all_toks = list(self.prepend_toks)\n",
    "        self.all_toks.extend(self.standard_toks)\n",
    "        for i in range((8 - (len(self.all_toks) % 8)) % 8):\n",
    "            self.all_toks.append(f\"<null_{i  + 1}>\")\n",
    "        self.all_toks.extend(self.append_toks)\n",
    "\n",
    "        self.tok_to_idx = {tok: i for i, tok in enumerate(self.all_toks)}\n",
    "\n",
    "        self.unk_idx = self.tok_to_idx[\"<unk>\"]\n",
    "        self.padding_idx = self.get_idx(\"<pad>\")\n",
    "        self.cls_idx = self.get_idx(\"<cls>\")\n",
    "        self.mask_idx = self.get_idx(\"<mask>\")\n",
    "        self.eos_idx = self.get_idx(\"<eos>\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_toks)\n",
    "\n",
    "    def get_idx(self, tok):\n",
    "        return self.tok_to_idx.get(tok, self.unk_idx)\n",
    "\n",
    "    def get_tok(self, ind):\n",
    "        return self.all_toks[ind]\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\"toks\": self.toks}\n",
    "\n",
    "    def get_batch_converter(self):\n",
    "        return BatchConverter(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        return cls(standard_toks=d[\"toks\"])\n",
    "\n",
    "\n",
    "class NewAlphabet(Alphabet):\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "    def get_batch_converter(self):\n",
    "        return BatchConverter(self.alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "usGAjru-WeLp"
   },
   "outputs": [],
   "source": [
    "# @title Fixed Max Length\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class FastaDataset(object):\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df[\"fasta\"][idx],self.data_df[\"solubility\"][idx],self.data_df[\"sid\"][idx]\n",
    "\n",
    "class BatchConverter(object):\n",
    "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, crop):\n",
    "        self.alphabet = alphabet\n",
    "        self.crop = crop\n",
    "\n",
    "    def __call__(self, raw_batch):\n",
    "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
    "        batch_size = len(raw_batch)\n",
    "        tokens = torch.empty((batch_size, MAX_LENGTH + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos)), dtype=torch.int64)\n",
    "        tokens.fill_(self.alphabet.padding_idx)\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        strs = []\n",
    "        targets = torch.zeros((batch_size,), dtype=torch.float32)\n",
    "        for i, (seq_str, target, label) in enumerate(raw_batch):\n",
    "            labels.append(label)\n",
    "            lengths.append(len(seq_str))\n",
    "            strs.append(seq_str)\n",
    "            targets[i] = target\n",
    "            if self.alphabet.prepend_bos:\n",
    "                tokens[i, 0] = self.alphabet.cls_idx\n",
    "            seq = torch.tensor([self.alphabet.get_idx(s) for s in seq_str], dtype=torch.int64)\n",
    "            tokens[i, int(self.alphabet.prepend_bos) : len(seq_str) + int(self.alphabet.prepend_bos)] = seq\n",
    "            if self.alphabet.append_eos:\n",
    "                tokens[i, len(seq_str) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx\n",
    "\n",
    "        non_pad_mask = ~tokens.eq(self.alphabet.padding_idx) &\\\n",
    "         ~tokens.eq(self.alphabet.cls_idx) &\\\n",
    "         ~tokens.eq(self.alphabet.eos_idx)# B, T\n",
    "            \n",
    "        return tokens, torch.tensor(lengths), non_pad_mask, targets, labels # dct_mat, idct_mat, \n",
    "\n",
    "class Alphabet(object):\n",
    "    prepend_toks = (\"<null_0>\", \"<pad>\", \"<eos>\", \"<unk>\")\n",
    "    append_toks = (\"<cls>\", \"<mask>\", \"<sep>\")\n",
    "    prepend_bos = True\n",
    "    append_eos = False\n",
    "\n",
    "    def __init__(self, standard_toks):\n",
    "        self.standard_toks = list(standard_toks)\n",
    "\n",
    "        self.all_toks = list(self.prepend_toks)\n",
    "        self.all_toks.extend(self.standard_toks)\n",
    "        for i in range((8 - (len(self.all_toks) % 8)) % 8):\n",
    "            self.all_toks.append(f\"<null_{i  + 1}>\")\n",
    "        self.all_toks.extend(self.append_toks)\n",
    "\n",
    "        self.tok_to_idx = {tok: i for i, tok in enumerate(self.all_toks)}\n",
    "\n",
    "        self.unk_idx = self.tok_to_idx[\"<unk>\"]\n",
    "        self.padding_idx = self.get_idx(\"<pad>\")\n",
    "        self.cls_idx = self.get_idx(\"<cls>\")\n",
    "        self.mask_idx = self.get_idx(\"<mask>\")\n",
    "        self.eos_idx = self.get_idx(\"<eos>\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_toks)\n",
    "\n",
    "    def get_idx(self, tok):\n",
    "        return self.tok_to_idx.get(tok, self.unk_idx)\n",
    "\n",
    "    def get_tok(self, ind):\n",
    "        return self.all_toks[ind]\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\"toks\": self.toks}\n",
    "\n",
    "    def get_batch_converter(self):\n",
    "        return BatchConverter(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        return cls(standard_toks=d[\"toks\"])\n",
    "\n",
    "\n",
    "class NewAlphabet(Alphabet):\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "    def get_batch_converter(self):\n",
    "        return BatchConverter(self.alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiGI4zueZ4Yj"
   },
   "source": [
    "Input should be a dataframe with these 3 columns: sid, solubility, fasta\n",
    "Based on the choice between dynamic and fixed max length, comment out the filtering based on sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4FAO0hcY-_F"
   },
   "outputs": [],
   "source": [
    "# @title Dataset\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "FASTA_FILE = \"../Datasets/PSI_Biology/pET_full_without_his_tag.fa\"\n",
    "LABELS_FILE = \"../Datasets/PSI_Biology/class.txt\"\n",
    "CLUSTERS_FILE = \"../Datasets/PSI_Biology/psi_biology_nesg_partitioning_wl_th025_amT.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(LABELS_FILE, delimiter=\"\\t\")\n",
    "labels_df.columns = [\"sid\", \"solubility\"]\n",
    "labels_df.solubility = labels_df.solubility -1\n",
    "\n",
    "fasta_dict = read_fasta(FASTA_FILE)\n",
    "fasta_df = pd.DataFrame(fasta_dict.items(), columns=['Accession', 'fasta'])\n",
    "fasta_df[\"sid\"] = fasta_df.Accession.apply(lambda x: x.split(\"_\")[0])\n",
    "print(len(fasta_df))\n",
    "\n",
    "data_df = labels_df.merge(fasta_df)\n",
    "\n",
    "clusters_df = pd.read_csv(CLUSTERS_FILE)\n",
    "clusters_df.columns = [\"sid\",\"priority\",\"label-val\",\"between_connectivity\",\"cluster\"]\n",
    "\n",
    "data_df = data_df.merge(clusters_df)\n",
    "\n",
    "print(len(data_df))\n",
    "newalphabet_train = NewAlphabet(alphabet)\n",
    "newalphabet_val = NewAlphabet(alphabet)\n",
    "\n",
    "def get_split(i):\n",
    "    train_df = data_df[data_df.cluster != i]\n",
    "\n",
    "    # Ignore if dynamic batching\n",
    "    train_df[\"lengths\"] = train_df[\"fasta\"].apply(lambda x: len(x))\n",
    "    train_df = train_df[train_df[\"lengths\"] <= MAX_LENGTH].reset_index(drop=True)\n",
    "    \n",
    "    X, y = np.stack(train_df[\"sid\"].to_numpy()), np.stack(train_df[\"solubility\"].to_numpy())\n",
    "    sss_tt = StratifiedShuffleSplit(n_splits=1, test_size=512, random_state=0)\n",
    "    \n",
    "    (split_train_idx, split_val_idx) = next(sss_tt.split(X, y))\n",
    "    split_train_df =  train_df.iloc[split_train_idx].reset_index(drop=True)\n",
    "    split_val_df = train_df.iloc[split_val_idx].reset_index(drop=True)\n",
    "    print(len(split_train_df))\n",
    "\n",
    "    train_dataset = FastaDataset(split_train_df)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "      train_dataset,\n",
    "      collate_fn=newalphabet_train.get_batch_converter(),\n",
    "      shuffle=True,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      num_workers=4,\n",
    "      #pin_memory=True,\n",
    "      drop_last=True)\n",
    "\n",
    "    val_dataset = FastaDataset(split_val_df)\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "      val_dataset,\n",
    "      collate_fn=newalphabet_val.get_batch_converter(),\n",
    "      #num_workers=4,\n",
    "      shuffle=False,\n",
    "        batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDD33TnwY_B8"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "path = \"./models/\"\n",
    "\n",
    "def train_model(idx):\n",
    "\n",
    "    train_dataloader, val_dataloader = get_next_split(idx)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.00,\n",
    "        patience=3,\n",
    "        verbose=False,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath=path,\n",
    "        filename= f\"{idx}\" + 'PSISplit-{epoch:02d}-{val_loss:.2f}',\n",
    "        period=1,\n",
    "        save_top_k=1,\n",
    "        save_last=False\n",
    "    )\n",
    "\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(max_epochs=3, \n",
    "                        check_val_every_n_epoch=1, \n",
    "                        default_root_dir=path + f\"{idx}\",\n",
    "                        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "                        precision=16,\n",
    "                        progress_bar_refresh_rate=1000,\n",
    "                        accumulate_grad_batches=4,\n",
    "                        gpus=1)\n",
    "    clf = ESMFinetune()\n",
    "    print(f\"Training clf {idx}\")\n",
    "    trainer.fit(clf, train_dataloader, val_dataloader)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEEoQk1BY_Ex"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  train_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Olz5yFY0Y_HC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVxBVX6UWePO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYVFdJ_UWjYu"
   },
   "source": [
    "# **TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zveLY2wmWeBa"
   },
   "outputs": [],
   "source": [
    "# @title Dynamic batching\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class FastaBatchedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.data_df = self.data_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df[\"fasta\"][idx], self.data_df[\"solubility\"][idx], self.data_df[\"sid\"][idx]\n",
    "\n",
    "    def get_batch_indices(self, toks_per_batch, extra_toks_per_seq=0):\n",
    "        sizes = [(len(s), i) for i, s in enumerate(self.data_df[\"fasta\"])]\n",
    "        sizes.sort()\n",
    "        batches = []\n",
    "        buf = []\n",
    "        max_len = 0\n",
    "\n",
    "        def _flush_current_buf():\n",
    "            nonlocal max_len, buf\n",
    "            if len(buf) == 0:\n",
    "                return\n",
    "            batches.append(buf)\n",
    "            buf = []\n",
    "            max_len = 0\n",
    "        start = 0\n",
    "        #start = random.randint(0, len(sizes))\n",
    "        for j in range(len(sizes)):\n",
    "            i = (start + j) % len(sizes)\n",
    "            sz = sizes[i][0]\n",
    "            idx = sizes[i][1]    \n",
    "            sz += extra_toks_per_seq\n",
    "            if (max(sz, max_len) * (len(buf) + 1) > toks_per_batch):\n",
    "                _flush_current_buf()\n",
    "            max_len = max(max_len, sz)\n",
    "            buf.append(idx)\n",
    "\n",
    "        _flush_current_buf()\n",
    "        return batches\n",
    "\n",
    "class BatchConverter(object):\n",
    "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "    def __call__(self, raw_batch):\n",
    "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
    "        batch_size = len(raw_batch)\n",
    "        #print(len(raw_batch[0]), raw_batch[1], raw_batch[2])\n",
    "        max_len = max(len(seq_str) for seq_str, _, _ in raw_batch)\n",
    "        tokens = torch.empty((batch_size, max_len + int(self.alphabet.prepend_bos) + \\\n",
    "            int(self.alphabet.append_eos)), dtype=torch.int64)\n",
    "        tokens.fill_(self.alphabet.padding_idx)\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        strs = []\n",
    "        targets = torch.zeros((batch_size,), dtype=torch.float32)\n",
    "        for i, (seq_str, target, label) in enumerate(raw_batch):\n",
    "            #seq_str = seq_str[1:]\n",
    "            labels.append(label)\n",
    "            lengths.append(len(seq_str))\n",
    "            strs.append(seq_str)\n",
    "            targets[i] = target\n",
    "            if self.alphabet.prepend_bos:\n",
    "                tokens[i, 0] = self.alphabet.cls_idx\n",
    "            seq = torch.tensor([self.alphabet.get_idx(s) for s in seq_str], dtype=torch.int64)\n",
    "            tokens[i, int(self.alphabet.prepend_bos) : len(seq_str) + int(self.alphabet.prepend_bos)] = seq\n",
    "            if self.alphabet.append_eos:\n",
    "                tokens[i, len(seq_str) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx\n",
    "        \n",
    "        non_pad_mask = ~tokens.eq(self.alphabet.padding_idx) &\\\n",
    "         ~tokens.eq(self.alphabet.cls_idx) &\\\n",
    "         ~tokens.eq(self.alphabet.eos_idx)# B, T\n",
    "\n",
    "        return tokens, torch.tensor(lengths), non_pad_mask, targets, labels\n",
    "\n",
    "class Alphabet(object):\n",
    "    prepend_toks = (\"<null_0>\", \"<pad>\", \"<eos>\", \"<unk>\")\n",
    "    append_toks = (\"<cls>\", \"<mask>\", \"<sep>\")\n",
    "    prepend_bos = True\n",
    "    append_eos = False\n",
    "\n",
    "    def __init__(self, standard_toks):\n",
    "        self.standard_toks = list(standard_toks)\n",
    "\n",
    "        self.all_toks = list(self.prepend_toks)\n",
    "        self.all_toks.extend(self.standard_toks)\n",
    "        for i in range((8 - (len(self.all_toks) % 8)) % 8):\n",
    "            self.all_toks.append(f\"<null_{i  + 1}>\")\n",
    "        self.all_toks.extend(self.append_toks)\n",
    "\n",
    "        self.tok_to_idx = {tok: i for i, tok in enumerate(self.all_toks)}\n",
    "\n",
    "        self.unk_idx = self.tok_to_idx[\"<unk>\"]\n",
    "        self.padding_idx = self.get_idx(\"<pad>\")\n",
    "        self.cls_idx = self.get_idx(\"<cls>\")\n",
    "        self.mask_idx = self.get_idx(\"<mask>\")\n",
    "        self.eos_idx = self.get_idx(\"<eos>\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_toks)\n",
    "\n",
    "    def get_idx(self, tok):\n",
    "        return self.tok_to_idx.get(tok, self.unk_idx)\n",
    "\n",
    "    def get_tok(self, ind):\n",
    "        return self.all_toks[ind]\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\"toks\": self.toks}\n",
    "\n",
    "    def get_batch_converter(self):\n",
    "        return BatchConverter(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        return cls(standard_toks=d[\"toks\"])\n",
    "\n",
    "\n",
    "class NewAlphabet(Alphabet):\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "    def get_batch_converter(self):\n",
    "        return BatchConverter(self.alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "im03jQtvWeEC"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, matthews_corrcoef, roc_curve\n",
    "import numpy as np\n",
    "optimal_thresholds = []\n",
    "\n",
    "def evaluate_split(split_i, test_df):\n",
    "    probs = np.stack(test_df[\"preds\"].to_numpy())\n",
    "    y_test = np.stack(test_df[\"solubility\"].to_numpy())\n",
    "\n",
    "    preds = probs>0.5\n",
    "    \n",
    "    # youden index\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test,net1_probs_test)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_thresholds.append(optimal_threshold)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    pre = precision_score(y_test, preds)\n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    print(f\"Fold{split_i}- Acc: {acc:.3f}, Pre: {pre:.3f}, MCC: {mcc:.3f}, AUC: {auc:.3f}\\n\")\n",
    "    return acc, pre, mcc, auc\n",
    "\n",
    "def predict(df, clf):\n",
    "    test_df = df\n",
    "    print(len(test_df))\n",
    "    newalphabet = NewAlphabet(alphabet)\n",
    "    embed_dataset = FastaBatchedDataset(test_df)\n",
    "    embed_batches = embed_dataset.get_batch_indices(MAX_TOKENS_PER_BATCH, extra_toks_per_seq=1)\n",
    "    embed_dataloader = torch.utils.data.DataLoader(embed_dataset, collate_fn=newalphabet.get_batch_converter(), batch_sampler=embed_batches)\n",
    "\n",
    "    embed_dict = {}\n",
    "    with torch.no_grad():\n",
    "      for i, (toks, lengths, np_mask, targets, labels) in enumerate(embed_dataloader):\n",
    "          x = torch.sigmoid(clf(toks.to(\"cuda\"), lengths.to(\"cuda\"), np_mask.to(\"cuda\"))).cpu().numpy()\n",
    "          for j in range(len(labels)):\n",
    "              if len(labels) == 1:\n",
    "                embed_dict[labels[j]] = x\n",
    "              else:\n",
    "                embed_dict[labels[j]] = x[j]\n",
    "\n",
    "    pred_df = pd.DataFrame(embed_dict.items(), columns=['sid', 'preds'])\n",
    "    test_df = test_df.merge(pred_df)\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "def get_dataset_split(split_i):\n",
    "    FASTA_FILE = \"../Datasets/PSI_Biology/pET_full_without_his_tag.fa\"\n",
    "    LABELS_FILE = \"../Datasets/PSI_Biology/class.txt\"\n",
    "    CLUSTERS_FILE = \"../Datasets/PSI_Biology/psi_biology_nesg_partitioning_wl_th025_amT.csv\"\n",
    "\n",
    "    labels_df = pd.read_csv(LABELS_FILE, delimiter=\"\\t\")\n",
    "    labels_df.columns = [\"sid\", \"solubility\"]\n",
    "    labels_df.solubility = labels_df.solubility -1\n",
    "\n",
    "    fasta_dict = read_fasta(FASTA_FILE)\n",
    "    fasta_df = pd.DataFrame(fasta_dict.items(), columns=['Accession', 'fasta'])\n",
    "    fasta_df[\"sid\"] = fasta_df.Accession.apply(lambda x: x.split(\"_\")[0])\n",
    "    print(len(fasta_df))\n",
    "\n",
    "    data_df = labels_df.merge(fasta_df)\n",
    "    clusters_df = pd.read_csv(CLUSTERS_FILE)\n",
    "    clusters_df.columns = [\"sid\",\"priority\",\"label-val\",\"between_connectivity\",\"cluster\"]\n",
    "    data_df = data_df.merge(clusters_df)\n",
    "    data_df = data_df[data_df.cluster == i].reset_index(drop=True)\n",
    "    return data_df\n",
    "\n",
    "def test_psi_split(split_i, clf):\n",
    "    # Input should be a dataframe with these 3 columns: sid, solubility, fasta\n",
    "    data_df = get_dataset_split(split_i)\n",
    "    pred_df = predict(data_df, clf)\n",
    "    evaluate_split(split_i, pred_df)\n",
    "    return pred_df\n",
    "\n",
    "def test_independent(split_i, clf):\n",
    "    data_df = pd.read_csv(\"../Datasets/NESG/NESG_testset.csv\")\n",
    "    pred_df = predict(data_df, clf)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ay5EBVgZ_vJ"
   },
   "outputs": [],
   "source": [
    "# cross validaiton\n",
    "accs = []\n",
    "pres = []\n",
    "mccs = []\n",
    "aucs = []\n",
    "\n",
    "for i in range(5):\n",
    "    path = f\"models/{i}PSISplit.ckpt\"\n",
    "    clf = ESMFinetune.load_from_checkpoint(path)\n",
    "    clf.eval().cuda()\n",
    "    acc, pre, mcc, auc = test_psi_split(i, clf)\n",
    "    accs.append(acc)\n",
    "    pres.append(pre)\n",
    "    mccs.append(mcc)\n",
    "    aucs.append(auc)\n",
    "\n",
    "print(f\"{round(np.array(accs).mean(), 2)} + {round(np.array(accs).std(), 2)}\" + \" & \"\n",
    "      f\"{round(np.array(pres).mean(), 2)} + {round(np.array(pres).std(), 2)}\" + \" & \"\n",
    "      f\"{round(np.array(mccs).mean(), 2)} + {round(np.array(mccs).std(), 2)}\" + \" & \"\n",
    "      f\"{round(np.array(aucs).mean(), 2)} + {round(np.array(aucs).std(), 2)}\" + \" \\\\\\\\ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gykvEMZPWeGf"
   },
   "outputs": [],
   "source": [
    "# independent validation\n",
    "preds_list = []\n",
    "for i in range(5):  \n",
    "  path = f\"models/{i}PSISplit.ckpt\"\n",
    "  clf = ESMFinetune.load_from_checkpoint(path)\n",
    "  clf.eval().cuda()\n",
    "  test_df = test_psi_split(i, clf)\n",
    "  preds_list.append(np.stack(test_df.preds.to_numpy()))\n",
    "\n",
    "\n",
    "probs = sum(preds_list) / 5\n",
    "y_test = np.stack(test_df[\"solubility\"].to_numpy())\n",
    "preds = probs > sum(optimal_thresholds)/5\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "pre = precision_score(y_test, preds)\n",
    "mcc = matthews_corrcoef(y_test, preds)\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(f\"Acc: {acc:.3f}, Pre: {pre:.3f}, MCC: {mcc:.3f}, AUC: {auc:.3f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "giHbgUFGWlwg",
    "bYVFdJ_UWjYu"
   ],
   "name": "SolubilityPrediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
